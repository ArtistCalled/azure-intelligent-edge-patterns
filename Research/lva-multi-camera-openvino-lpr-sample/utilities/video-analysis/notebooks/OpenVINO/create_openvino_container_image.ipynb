{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Local Docker Image\n",
    "In this section, we will create an IoT Edge module, a Docker container image with an HTTP web server that has a scoring REST endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../common')\n",
    "from env_variables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Web Application & Inference Server for Our ML Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To change the inference model that will be used in this sample, change the variable `IS_MODEL_NAME` based on your preferred model. Note that the model must be downloaded, as instructed in the [previous section](create_openvino_inference_engine.ipynb). The default model is         \"person-vehicle-bike-detection-crossroad-1016\".\n",
    "\n",
    "2. The variable assignment of `IS_TARGET_DEVICE` indicates what type of hardware acceleration you would like to use on your IoT Edge device. You can choose from the following choices:\n",
    "    * \"CPU\" for Intel® CPU acceleration  \n",
    "    * \"MYRIAD\" for Intel® VPU acceleration\n",
    "    * \"GPU\" for Intel® GPU acceleration\n",
    "    * \"FPGA\" for Intel® FPGA acceleration\n",
    "\n",
    "    Change the variable `IS_TARGET_DEVICE` as needed; the default is \"CPU\". \n",
    "\n",
    "3. The variable assignment of `IS_MODEL_PRECISION` indicates what type of model precision you would like to use. The default is \"FP32\" for this sample. However, please [check the Intel documentation](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_Supported_Devices.html) for which output precision is supported for your desired hardware acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/app.py\n",
    "import threading\n",
    "import cv2\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import linecache\n",
    "from score import MLModel, PrintGetExceptionDetails\n",
    "from flask import Flask, request, Response\n",
    "\n",
    "# Initial settings of AI model\n",
    "#IS_MODEL_NAME = \"person-vehicle-bike-detection-crossroad-1016\" # see MLModel class for full list of models and other possibilities\n",
    "IS_MODEL_NAME = \"vehicle-license-plate-detection-barrier-0106\" # see MLModel class for full list of models and other possibilities\n",
    "IS_TARGET_DEVICE = \"CPU\"\n",
    "IS_MODEL_PRECISION = \"FP32\"\n",
    "\n",
    "app = Flask(__name__)\n",
    "inferenceEngine = MLModel(    modelName=IS_MODEL_NAME, \n",
    "                                modelPrecision=IS_MODEL_PRECISION, \n",
    "                                targetDev=IS_TARGET_DEVICE, \n",
    "                            )\n",
    "\n",
    "@app.route(\"/score\", methods = ['POST'])\n",
    "def scoreRRS():\n",
    "    global inferenceEngine\n",
    "\n",
    "    try:\n",
    "        # get request as byte stream\n",
    "        reqBody = request.get_data(False)\n",
    "\n",
    "        # convert from byte stream\n",
    "        inMemFile = io.BytesIO(reqBody)\n",
    "\n",
    "        # load a sample image\n",
    "        inMemFile.seek(0)\n",
    "        fileBytes = np.asarray(bytearray(inMemFile.read()), dtype=np.uint8)\n",
    "        cvImage = cv2.imdecode(fileBytes, cv2.IMREAD_COLOR)\n",
    "\n",
    "        # call scoring function\n",
    "        detectedObjects = inferenceEngine.score(cvImage)            \n",
    "\n",
    "        if len(detectedObjects) > 0:\n",
    "            respBody = {                    \n",
    "                        \"inferences\" : detectedObjects\n",
    "                    }\n",
    "\n",
    "            respBody = json.dumps(respBody)\n",
    "            \n",
    "            logging.info(\"[AI EXT] Sending response.\")\n",
    "            return Response(respBody, status= 200, mimetype ='application/json')\n",
    "        else:\n",
    "            logging.info(\"[AI EXT] Sending empty response.\")\n",
    "            return Response(status= 204)\n",
    "\n",
    "    except:\n",
    "        PrintGetExceptionDetails()\n",
    "        return Response(response='Exception occured while processing the image.', status=500)\n",
    "    \n",
    "@app.route(\"/\")\n",
    "def healthy():\n",
    "    return \"Healthy\"\n",
    "\n",
    "# About\n",
    "@app.route('/about', methods = ['GET'])\n",
    "def about_request():\n",
    "    global inferenceEngine\n",
    "    return inferenceEngine.about()\n",
    "\n",
    "if __name__ == \"__main__\":      \n",
    "    app.run(host='127.0.0.1', port=5444)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, 5444 is the internal port of the webserver app that listens the requests. Next, we will map it to different ports to expose it externally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/wsgi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/wsgi.py\n",
    "from app import app as application\n",
    "\n",
    "def create():\n",
    "    application.run(host='127.0.0.1', port=5444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(os.path.join(isSolutionPath, \"nginx\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exposed port of the web app is now 5001, while the internal one is still 5444."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/nginx/app\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/nginx/app\n",
    "server {\n",
    "    listen 5001;\n",
    "    server_name _;\n",
    " \n",
    "    location / {\n",
    "    include proxy_params;\n",
    "    proxy_pass http://127.0.0.1:5444;\n",
    "    proxy_connect_timeout 5000s;\n",
    "    proxy_read_timeout 5000s;\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/gunicorn_logging.conf\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/gunicorn_logging.conf\n",
    "\n",
    "[loggers]\n",
    "keys=root, gunicorn.error\n",
    "\n",
    "[handlers]\n",
    "keys=console\n",
    "\n",
    "[formatters]\n",
    "keys=json\n",
    "\n",
    "[logger_root]\n",
    "level=INFO\n",
    "handlers=console\n",
    "\n",
    "[logger_gunicorn.error]\n",
    "level=ERROR\n",
    "handlers=console\n",
    "propagate=0\n",
    "qualname=gunicorn.error\n",
    "\n",
    "[handler_console]\n",
    "class=StreamHandler\n",
    "formatter=json\n",
    "args=(sys.stdout, )\n",
    "\n",
    "[formatter_json]\n",
    "class=jsonlogging.JSONFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/kill_supervisor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/kill_supervisor.py\n",
    "import sys\n",
    "import os\n",
    "import signal\n",
    "\n",
    "def write_stdout(s):\n",
    "    sys.stdout.write(s)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# this function is modified from the code and knowledge found here: http://supervisord.org/events.html#example-event-listener-implementation\n",
    "def main():\n",
    "    while 1:\n",
    "        write_stdout('[AI EXT] READY\\n')\n",
    "        # wait for the event on stdin that supervisord will send\n",
    "        line = sys.stdin.readline()\n",
    "        write_stdout('[AI EXT] Terminating supervisor with this event: ' + line);\n",
    "        try:\n",
    "            # supervisord writes its pid to its file from which we read it here, see supervisord.conf\n",
    "            pidfile = open('/tmp/supervisord.pid','r')\n",
    "            pid = int(pidfile.readline());\n",
    "            os.kill(pid, signal.SIGQUIT)\n",
    "        except Exception as e:\n",
    "            write_stdout('[AI EXT] Could not terminate supervisor: ' + e.strerror + '\\n')\n",
    "            write_stdout('[AI EXT] RESULT 2\\nOK')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(os.path.join(isSolutionPath, \"etc\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/etc/supervisord.conf\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/etc/supervisord.conf \n",
    "[supervisord]\n",
    "logfile=/tmp/supervisord.log ; (main log file;default $CWD/supervisord.log)\n",
    "logfile_maxbytes=50MB        ; (max main logfile bytes b4 rotation;default 50MB)\n",
    "logfile_backups=10           ; (num of main logfile rotation backups;default 10)\n",
    "loglevel=info                ; (log level;default info; others: debug,warn,trace)\n",
    "pidfile=/tmp/supervisord.pid ; (supervisord pidfile;default supervisord.pid)\n",
    "nodaemon=true                ; (start in foreground if true;default false)\n",
    "minfds=1024                  ; (min. avail startup file descriptors;default 1024)\n",
    "minprocs=200                 ; (min. avail process descriptors;default 200)\n",
    "\n",
    "environment=LD_LIBRARY_PATH=%(ENV_LD_LIBRARY_PATH)s,INTEL_CVSDK_DIR=%(ENV_INTEL_CVSDK_DIR)s,OpenCV_DIR=%(ENV_OpenCV_DIR)s,InferenceEngine_DIR=%(ENV_InferenceEngine_DIR)s,PYTHONPATH=%(ENV_PYTHONPATH)s,INTEL_OPENVINO_DIR=%(ENV_INTEL_OPENVINO_DIR)s,PATH=%(ENV_PATH)s,HDDL_INSTALL_DIR=%(ENV_HDDL_INSTALL_DIR)s,INTEL_OPENVINO_DIR=%(ENV_INTEL_OPENVINO_DIR)s,PATH=%(ENV_PATH)s\n",
    "\n",
    "[program:gunicorn]\n",
    "command=bash -c \"gunicorn --workers 1 -m 007 --timeout 100000 --capture-output --error-logfile - --log-level debug --log-config gunicorn_logging.conf \\\"wsgi:create()\\\"\"\n",
    "directory=/isserver\n",
    "redirect_stderr=true\n",
    "stdout_logfile =/dev/stdout\n",
    "stdout_logfile_maxbytes=0\n",
    "startretries=2\n",
    "startsecs=20\n",
    "\n",
    "[program:nginx]\n",
    "command=/usr/sbin/nginx -g \"daemon off;\"\n",
    "startretries=2\n",
    "startsecs=5\n",
    "priority=3\n",
    "\n",
    "[eventlistener:program_exit]\n",
    "command=python kill_supervisor.py\n",
    "directory=/isserver\n",
    "events=PROCESS_STATE_FATAL\n",
    "priority=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/requirements.txt\n",
    "pillow<7.0.0\n",
    "click==6.7\n",
    "configparser==3.5.0\n",
    "Flask==0.12.2\n",
    "gunicorn==19.6.0\n",
    "json-logging-py==0.2\n",
    "MarkupSafe==1.0\n",
    "olefile==0.44\n",
    "requests==2.12.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Docker File to Containerize the ML Solution and Web App Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red; font-weight: bold; font-size:1.1em;\"> [!IMPORTANT] </span>  \n",
    "\n",
    "> The OpenVINO™ Toolkit is a licenced software. To ensure that you are using the latest version of the OpenVINO™ Toolkit, follow these instructions to obtain a licensed download link:  \n",
    "\n",
    "> 1) Go to the [Intel donwload link](https://software.intel.com/en-us/openvino-toolkit/choose-download/free-download-linux) for the OpenVINO™ Toolkit\n",
    "\n",
    "> 2) Click on the \"Register & Download\" button  \n",
    "\n",
    "> <img src=\"../../../../images/_openvino_img_03_001.jpg\" width=400 alt=\"> Figure: Register & Download.\"/>  \n",
    "\n",
    "> 3) Fill in the form and click submit \n",
    "\n",
    "> <img src=\"../../../../images/_openvino_img_03_002.jpg\" width=400 alt=\"> Figure: Fill the form.\"/>  \n",
    "\n",
    "> 4) Over the \"Full Package\" link, right click and get the link which should look like something:  \n",
    "    http://registrationcenter-download.intel.com/akdlm/irc_nas/<SOMECODE\\>/l_openvino_toolkit_p_2020.3.194.tgz  \n",
    "    \n",
    "> <img src=\"../../../../images/_openvino_img_03_003.jpg\" width=400 alt=\"> Figure: Download link.\"/>  \n",
    "\n",
    "> 5) In the below cell, set the value of variable \"openVinoToolkitDownloadLink\" to the download link you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As described above, set the value of variable \"openVinoToolkitDownloadLink\" to the download link you have (below is sample URI, just remove it and use your own address)\n",
    "\n",
    "# openVinoToolkitDownloadLink = \"http://registrationcenter-download.intel.com/akdlm/irc_nas/<SOMECODE>/l_openvino_toolkit_p_2020.3.194.tgz\"\n",
    "openVinoToolkitDownloadLink = \"http://registrationcenter-download.intel.com/akdlm/irc_nas/16670/l_openvino_toolkit_p_2020.3.194.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inferenceserver/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile $isSolutionPath/Dockerfile\n",
    "\n",
    "FROM ubuntu:18.04\n",
    "\n",
    "USER root\n",
    "\n",
    "ARG WORK_DIR=/isserver\n",
    "ENV WORK_DIR ${WORK_DIR}\n",
    "ENV PATH /opt/miniconda/bin:${PATH}\n",
    "\n",
    "RUN mkdir -p ${WORK_DIR}\n",
    "\n",
    "WORKDIR ${WORK_DIR}\n",
    "\n",
    "#\n",
    "# Install base\n",
    "#\n",
    "RUN apt-get update &&\\\n",
    "    apt-get install -y --no-install-recommends \\\n",
    "        # Essentials\n",
    "        wget \\\n",
    "        locales \\\n",
    "        # Python environment\n",
    "        python3 \\\n",
    "        python3-setuptools &&\\\n",
    "    #\n",
    "    # Dependencies: conda\n",
    "    wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-4.5.11-Linux-x86_64.sh -O ${WORK_DIR}/miniconda.sh --no-check-certificate &&\\ \n",
    "    /bin/bash ${WORK_DIR}/miniconda.sh -b -p /opt/miniconda &&\\\n",
    "    #\n",
    "    # Cleaning\n",
    "    /opt/miniconda/bin/conda clean -ya &&\\\n",
    "    rm -rf /opt/miniconda/pkgs &&\\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "#\n",
    "# Install OpenVINO™\n",
    "#\n",
    "#COPY l_openvino_toolkit_p_2020.1.023.tgz ${WORK_DIR}\n",
    "RUN apt-get update &&\\\n",
    "    apt-get install -y --no-install-recommends \\\n",
    "        # Essentials\n",
    "        cpio \\\n",
    "        udev \\\n",
    "        unzip \\\n",
    "        autoconf \\\n",
    "        automake \\\n",
    "        libtool\n",
    "\n",
    "RUN wget --quiet IS_OPENVINO_TOOLKIT_DOWNLOAD_LINK -O ${WORK_DIR}/l_openvino_toolkit_p_2020.1.023.tgz &&\\\n",
    "    pattern=\"COMPONENTS=DEFAULTS\" &&\\\n",
    "    replacement=\"COMPONENTS=intel-openvino-ie-sdk-ubuntu-bionic__x86_64;intel-openvino-ie-rt-cpu-ubuntu-bionic__x86_64;intel-openvino-ie-rt-vpu-ubuntu-bionic__x86_64;intel-openvino-opencv-lib-ubuntu-bionic__x86_64\" &&\\\n",
    "    tar -xzf l_openvino_toolkit*.tgz &&\\\n",
    "    rm -rf l_openvino_toolkit*.tgz &&\\\n",
    "    cd l_openvino_toolkit* &&\\\n",
    "    sed -i \"s/$pattern/$replacement/\" silent.cfg &&\\\n",
    "    sed -i \"s/decline/accept/g\" silent.cfg &&\\\n",
    "    /bin/bash ./install.sh -s silent.cfg &&\\\n",
    "    cd - &&\\\n",
    "    cd /opt/intel/openvino/install_dependencies &&\\\n",
    "    /bin/bash ./install_openvino_dependencies.sh &&\\\n",
    "    # setup environment variables\n",
    "    echo \"source /opt/intel/openvino/bin/setupvars.sh\" >> /root/.bashrc &&\\\n",
    "    #\n",
    "    # Cleaning\n",
    "    cd ${WORK_DIR} &&\\\n",
    "    rm -rf * &&\\\n",
    "    /opt/miniconda/bin/conda clean -ya &&\\\n",
    "    rm -rf /opt/miniconda/pkgs &&\\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "#\n",
    "# Set environment variables as in ${INTEL_OPENVINO_DIR}/bin/setupvars.sh\n",
    "ENV INTEL_OPENVINO_DIR /opt/intel/openvino\n",
    "ENV LD_LIBRARY_PATH ${INTEL_OPENVINO_DIR}/opencv/lib:${INTEL_OPENVINO_DIR}/deployment_tools/ngraph/lib:/opt/intel/opencl:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/hddl/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/gna/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/mkltiny_lnx/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/tbb/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/lib/intel64\n",
    "ENV INTEL_CVSDK_DIR ${INTEL_OPENVINO_DIR}\n",
    "ENV OpenCV_DIR ${INTEL_OPENVINO_DIR}/opencv/cmake\n",
    "ENV InferenceEngine_DIR ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/share\n",
    "ENV PYTHONPATH ${INTEL_OPENVINO_DIR}/python/python3.7:${INTEL_OPENVINO_DIR}/python/python3:${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/accuracy_checker:${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer\n",
    "ENV PATH ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer:/opt/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:${PATH}\n",
    "ENV HDDL_INSTALL_DIR ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/hddl\n",
    "\n",
    "#\n",
    "# Exclude UDEV by rebuilding libusb without UDEV support\n",
    "RUN cp ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/97-myriad-usbboot.rules /etc/udev/rules.d/ &&\\\n",
    "    ldconfig &&\\\n",
    "    cd /opt && wget --quiet --no-check-certificate http://github.com/libusb/libusb/archive/v1.0.22.zip -O /opt/v1.0.22.zip &&\\\n",
    "    unzip v1.0.22.zip && cd libusb-1.0.22 &&\\\n",
    "    ./bootstrap.sh &&\\\n",
    "    ./configure --disable-udev --enable-shared &&\\\n",
    "    make -j4\n",
    "\n",
    "RUN apt-get update &&\\\n",
    "    apt-get install -y --no-install-recommends libusb-1.0-0-dev &&\\\n",
    "    cd /opt &&\\\n",
    "    rm -rf /var/lib/apt/lists/* &&\\\n",
    "    cd /opt/libusb-1.0.22/libusb &&\\\n",
    "    /bin/mkdir -p '/usr/local/lib' &&\\\n",
    "    /bin/bash ../libtool --mode=install /usr/bin/install -c libusb-1.0.la '/usr/local/lib' &&\\\n",
    "    /bin/mkdir -p '/usr/local/include/libusb-1.0' &&\\\n",
    "    /usr/bin/install -c -m 644 libusb.h '/usr/local/include/libusb-1.0' &&\\\n",
    "    /bin/mkdir -p '/usr/local/lib/pkgconfig' &&\\\n",
    "    cd /opt/libusb-1.0.22/ &&\\\n",
    "    /usr/bin/install -c -m 644 libusb-1.0.pc '/usr/local/lib/pkgconfig' &&\\\n",
    "    ldconfig\n",
    "\n",
    "#\n",
    "# Install ML solution\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    nginx \\\n",
    "    supervisor &&\\\n",
    "    pip install \\\n",
    "        numpy \\\n",
    "        azure-iot-device\n",
    "        \n",
    "ADD . ${WORK_DIR}\n",
    "ADD etc /etc\n",
    "\n",
    "RUN rm -rf /var/lib/apt/lists/* &&\\\n",
    "    rm /etc/nginx/sites-enabled/default &&\\\n",
    "    cp ${WORK_DIR}/nginx/app /etc/nginx/sites-available/ &&\\\n",
    "    ln -s /etc/nginx/sites-available/app /etc/nginx/sites-enabled/ &&\\\n",
    "    pip install -r ${WORK_DIR}/requirements.txt &&\\\n",
    "    /opt/miniconda/bin/conda clean -ya &&\\\n",
    "    rm -rf /opt/miniconda/pkgs &&\\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "EXPOSE 5001\n",
    "CMD [\"supervisord\", \"-c\", \"/isserver/etc/supervisord.conf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Docker file with custom environment variable: IoT Edge device's connection string\n",
    "filePath = isSolutionPath+\"/Dockerfile\"\n",
    "file = open(filePath)\n",
    "dockerFileTemplate = file.read()\n",
    "dockerFileTemplate = dockerFileTemplate.replace(\"IS_OPENVINO_TOOLKIT_DOWNLOAD_LINK\", \"\\\"\"+openVinoToolkitDownloadLink+\"\\\"\")\n",
    "\n",
    "with open(filePath, 'wt', encoding='utf-8') as outputFile:\n",
    "    outputFile.write(dockerFileTemplate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Local Docker Image\n",
    "Finally, we will create a Docker image locally. We will later host the image in a container registry like Docker Hub, Azure Container Registry, or a local registry.\n",
    "\n",
    "To run the following code snippet, you must have the pre-requisities mentioned in [the requirements page](../common/requirements.md). Most notably, we are running the `docker` command without `sudo`.\n",
    "\n",
    "> <span>[!WARNING]</span>\n",
    "> Please ensure that Docker is running before executing the cell below. Execution of the cell below may take several minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  102.8MB\n",
      "Step 1/26 : FROM ubuntu:18.04\n",
      " ---> d27b9ffc5667\n",
      "Step 2/26 : USER root\n",
      " ---> Using cache\n",
      " ---> 30fea6fec12d\n",
      "Step 3/26 : ARG WORK_DIR=/isserver\n",
      " ---> Using cache\n",
      " ---> e051f506fe89\n",
      "Step 4/26 : ENV WORK_DIR ${WORK_DIR}\n",
      " ---> Using cache\n",
      " ---> 5800225e4748\n",
      "Step 5/26 : ENV PATH /opt/miniconda/bin:${PATH}\n",
      " ---> Using cache\n",
      " ---> 5d2b2bed46a5\n",
      "Step 6/26 : RUN mkdir -p ${WORK_DIR}\n",
      " ---> Using cache\n",
      " ---> 1d82213511af\n",
      "Step 7/26 : WORKDIR ${WORK_DIR}\n",
      " ---> Using cache\n",
      " ---> b83b980f1afa\n",
      "Step 8/26 : RUN apt-get update &&    apt-get install -y --no-install-recommends         wget         locales         python3         python3-setuptools &&    wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-4.5.11-Linux-x86_64.sh -O ${WORK_DIR}/miniconda.sh --no-check-certificate &&    /bin/bash ${WORK_DIR}/miniconda.sh -b -p /opt/miniconda &&    /opt/miniconda/bin/conda clean -ya &&    rm -rf /opt/miniconda/pkgs &&    rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> db424a27336c\n",
      "Step 9/26 : RUN apt-get update &&    apt-get install -y --no-install-recommends         cpio         udev         unzip         autoconf         automake         libtool\n",
      " ---> Using cache\n",
      " ---> b7f84eca0df8\n",
      "Step 10/26 : RUN wget --quiet \"http://registrationcenter-download.intel.com/akdlm/irc_nas/16670/l_openvino_toolkit_p_2020.3.194.tgz\" -O ${WORK_DIR}/l_openvino_toolkit_p_2020.1.023.tgz &&    pattern=\"COMPONENTS=DEFAULTS\" &&    replacement=\"COMPONENTS=intel-openvino-ie-sdk-ubuntu-bionic__x86_64;intel-openvino-ie-rt-cpu-ubuntu-bionic__x86_64;intel-openvino-ie-rt-vpu-ubuntu-bionic__x86_64;intel-openvino-opencv-lib-ubuntu-bionic__x86_64\" &&    tar -xzf l_openvino_toolkit*.tgz &&    rm -rf l_openvino_toolkit*.tgz &&    cd l_openvino_toolkit* &&    sed -i \"s/$pattern/$replacement/\" silent.cfg &&    sed -i \"s/decline/accept/g\" silent.cfg &&    /bin/bash ./install.sh -s silent.cfg &&    cd - &&    cd /opt/intel/openvino/install_dependencies &&    /bin/bash ./install_openvino_dependencies.sh &&    echo \"source /opt/intel/openvino/bin/setupvars.sh\" >> /root/.bashrc &&    cd ${WORK_DIR} &&    rm -rf * &&    /opt/miniconda/bin/conda clean -ya &&    rm -rf /opt/miniconda/pkgs &&    rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> d975c99acd42\n",
      "Step 11/26 : ENV INTEL_OPENVINO_DIR /opt/intel/openvino\n",
      " ---> Using cache\n",
      " ---> 03a6921eefaa\n",
      "Step 12/26 : ENV LD_LIBRARY_PATH ${INTEL_OPENVINO_DIR}/opencv/lib:${INTEL_OPENVINO_DIR}/deployment_tools/ngraph/lib:/opt/intel/opencl:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/hddl/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/gna/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/mkltiny_lnx/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/tbb/lib:${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/lib/intel64\n",
      " ---> Using cache\n",
      " ---> ca62e51b81b9\n",
      "Step 13/26 : ENV INTEL_CVSDK_DIR ${INTEL_OPENVINO_DIR}\n",
      " ---> Using cache\n",
      " ---> 702ccc0b663b\n",
      "Step 14/26 : ENV OpenCV_DIR ${INTEL_OPENVINO_DIR}/opencv/cmake\n",
      " ---> Using cache\n",
      " ---> 3165ed2f077f\n",
      "Step 15/26 : ENV InferenceEngine_DIR ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/share\n",
      " ---> Using cache\n",
      " ---> dcb08ae895fb\n",
      "Step 16/26 : ENV PYTHONPATH ${INTEL_OPENVINO_DIR}/python/python3.7:${INTEL_OPENVINO_DIR}/python/python3:${INTEL_OPENVINO_DIR}/deployment_tools/open_model_zoo/tools/accuracy_checker:${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer\n",
      " ---> Using cache\n",
      " ---> 6c12737a6d0f\n",
      "Step 17/26 : ENV PATH ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer:/opt/miniconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:${PATH}\n",
      " ---> Using cache\n",
      " ---> 9de6dd53de50\n",
      "Step 18/26 : ENV HDDL_INSTALL_DIR ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/hddl\n",
      " ---> Using cache\n",
      " ---> 3c69fd36d1ec\n",
      "Step 19/26 : RUN cp ${INTEL_OPENVINO_DIR}/deployment_tools/inference_engine/external/97-myriad-usbboot.rules /etc/udev/rules.d/ &&    ldconfig &&    cd /opt && wget --quiet --no-check-certificate http://github.com/libusb/libusb/archive/v1.0.22.zip -O /opt/v1.0.22.zip &&    unzip v1.0.22.zip && cd libusb-1.0.22 &&    ./bootstrap.sh &&    ./configure --disable-udev --enable-shared &&    make -j4\n",
      " ---> Using cache\n",
      " ---> 22dc66492665\n",
      "Step 20/26 : RUN apt-get update &&    apt-get install -y --no-install-recommends libusb-1.0-0-dev &&    cd /opt &&    rm -rf /var/lib/apt/lists/* &&    cd /opt/libusb-1.0.22/libusb &&    /bin/mkdir -p '/usr/local/lib' &&    /bin/bash ../libtool --mode=install /usr/bin/install -c libusb-1.0.la '/usr/local/lib' &&    /bin/mkdir -p '/usr/local/include/libusb-1.0' &&    /usr/bin/install -c -m 644 libusb.h '/usr/local/include/libusb-1.0' &&    /bin/mkdir -p '/usr/local/lib/pkgconfig' &&    cd /opt/libusb-1.0.22/ &&    /usr/bin/install -c -m 644 libusb-1.0.pc '/usr/local/lib/pkgconfig' &&    ldconfig\n",
      " ---> Using cache\n",
      " ---> c7fa9f6e4a52\n",
      "Step 21/26 : RUN apt-get update && apt-get install -y --no-install-recommends     nginx     supervisor &&    pip install         numpy         azure-iot-device\n",
      " ---> Using cache\n",
      " ---> e22257b5af4c\n",
      "Step 22/26 : ADD . ${WORK_DIR}\n",
      " ---> f1a45b54dfa0\n",
      "Step 23/26 : ADD etc /etc\n",
      " ---> 702319f5db7a\n",
      "Step 24/26 : RUN rm -rf /var/lib/apt/lists/* &&    rm /etc/nginx/sites-enabled/default &&    cp ${WORK_DIR}/nginx/app /etc/nginx/sites-available/ &&    ln -s /etc/nginx/sites-available/app /etc/nginx/sites-enabled/ &&    pip install -r ${WORK_DIR}/requirements.txt &&    /opt/miniconda/bin/conda clean -ya &&    rm -rf /opt/miniconda/pkgs &&    rm -rf /var/lib/apt/lists/*\n",
      " ---> Running in d4d2043ed922\n",
      "Collecting pillow<7.0.0 (from -r /isserver/requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/3f/03375124676ab49ca6e6917c0f1f663afb8354d5d24e12f4fe4587a39ae2/Pillow-6.2.2-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
      "Collecting click==6.7 (from -r /isserver/requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl (71kB)\n",
      "Collecting configparser==3.5.0 (from -r /isserver/requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/7c/69/c2ce7e91c89dc073eb1aa74c0621c3eefbffe8216b3f9af9d3885265c01c/configparser-3.5.0.tar.gz\n",
      "Collecting Flask==0.12.2 (from -r /isserver/requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/77/32/e3597cb19ffffe724ad4bf0beca4153419918e7fa4ba6a34b04ee4da3371/Flask-0.12.2-py2.py3-none-any.whl (83kB)\n",
      "Collecting gunicorn==19.6.0 (from -r /isserver/requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/72/de/ec28a64885e0b390063379cca601b60b1f9e51367e0c76030ac8a5cddd5e/gunicorn-19.6.0-py2.py3-none-any.whl (114kB)\n",
      "Collecting json-logging-py==0.2 (from -r /isserver/requirements.txt (line 6))\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/e1/46c70eebf216b830867c4896ee678cb7f1b28bb68a2810c7e9a811cecfbc/json-logging-py-0.2.tar.gz\n",
      "Collecting MarkupSafe==1.0 (from -r /isserver/requirements.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/4d/de/32d741db316d8fdb7680822dd37001ef7a448255de9699ab4bfcbdf4172b/MarkupSafe-1.0.tar.gz\n",
      "Collecting olefile==0.44 (from -r /isserver/requirements.txt (line 8))\n",
      "  Downloading https://files.pythonhosted.org/packages/35/17/c15d41d5a8f8b98cc3df25eb00c5cee76193114c78e5674df6ef4ac92647/olefile-0.44.zip (74kB)\n",
      "Collecting requests==2.12.3 (from -r /isserver/requirements.txt (line 9))\n",
      "  Downloading https://files.pythonhosted.org/packages/84/68/f0acceafe80354aa9ff4ae49de0572d27929b6d262f0c55196424eb86b2f/requests-2.12.3-py2.py3-none-any.whl (575kB)\n",
      "Collecting Werkzeug>=0.7 (from Flask==0.12.2->-r /isserver/requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB)\n",
      "Collecting Jinja2>=2.4 (from Flask==0.12.2->-r /isserver/requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/30/9e/f663a2aa66a09d838042ae1a2c5659828bb9b41ea3a6efa20a20fd92b121/Jinja2-2.11.2-py2.py3-none-any.whl (125kB)\n",
      "Collecting itsdangerous>=0.21 (from Flask==0.12.2->-r /isserver/requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: configparser, json-logging-py, MarkupSafe, olefile\n",
      "  Running setup.py bdist_wheel for configparser: started\n",
      "  Running setup.py bdist_wheel for configparser: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/61/79/424ef897a2f3b14684a7de5d89e8600b460b89663e6ce9d17c\n",
      "  Running setup.py bdist_wheel for json-logging-py: started\n",
      "  Running setup.py bdist_wheel for json-logging-py: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/2e/1c/c638b7589610d8b9358a6e5eb008edacb8b3e9b6d1edc9479f\n",
      "  Running setup.py bdist_wheel for MarkupSafe: started\n",
      "  Running setup.py bdist_wheel for MarkupSafe: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/33/56/20/ebe49a5c612fffe1c5a632146b16596f9e64676768661e4e46\n",
      "  Running setup.py bdist_wheel for olefile: started\n",
      "  Running setup.py bdist_wheel for olefile: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/c4/19/76/61fc7929d808e51567aff23036ca5fe6ba8336ad0559ca6a27\n",
      "Successfully built configparser json-logging-py MarkupSafe olefile\n",
      "\u001b[91mazure-iot-device 2.1.4 has requirement requests<3.0.0,>=2.20.0, but you'll have requests 2.12.3 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: pillow, click, configparser, Werkzeug, MarkupSafe, Jinja2, itsdangerous, Flask, gunicorn, json-logging-py, olefile, requests\n",
      "  Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "Successfully installed Flask-0.12.2 Jinja2-2.11.2 MarkupSafe-1.0 Werkzeug-1.0.1 click-6.7 configparser-3.5.0 gunicorn-19.6.0 itsdangerous-1.1.0 json-logging-py-0.2 olefile-0.44 pillow-6.2.2 requests-2.12.3\n",
      "\u001b[91mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mCache location: \n",
      "There are no tarballs to remove\n",
      "Cache location: \n",
      "There are no unused packages to remove\n",
      "source cache (/opt/miniconda/conda-bld/src_cache)\n",
      "Size:                                           0 B\n",
      "\n",
      "git cache (/opt/miniconda/conda-bld/git_cache)\n",
      "Size:                                           0 B\n",
      "\n",
      "hg cache (/opt/miniconda/conda-bld/hg_cache)\n",
      "Size:                                           0 B\n",
      "\n",
      "svn cache (/opt/miniconda/conda-bld/svn_cache)\n",
      "Size:                                           0 B\n",
      "\n",
      "Total:                                          0 B\n",
      "Removing /opt/miniconda/conda-bld/src_cache\n",
      "Removing /opt/miniconda/conda-bld/git_cache\n",
      "Removing /opt/miniconda/conda-bld/hg_cache\n",
      "Removing /opt/miniconda/conda-bld/svn_cache\n",
      "Removing intermediate container d4d2043ed922\n",
      " ---> 0320facf313b\n",
      "Step 25/26 : EXPOSE 5001\n",
      " ---> Running in 823bb829cf6e\n",
      "Removing intermediate container 823bb829cf6e\n",
      " ---> afdec2003a10\n",
      "Step 26/26 : CMD [\"supervisord\", \"-c\", \"/isserver/etc/supervisord.conf\"]\n",
      " ---> Running in bfb2c2cf77a4\n",
      "Removing intermediate container bfb2c2cf77a4\n",
      " ---> 6d2c6465d389\n",
      "Successfully built 6d2c6465d389\n",
      "Successfully tagged lvasample5d672faimodule:latest\n"
     ]
    }
   ],
   "source": [
    "!sudo docker build -t $containerImageName --file ./$isSolutionPath/Dockerfile ./$isSolutionPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "If all the code cells above have successfully finished running, return to the Readme page to continue.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
