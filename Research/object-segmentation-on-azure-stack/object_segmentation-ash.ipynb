{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Segmenation on Azure Stack Hub Clusters\n",
    "\n",
    "For this tutorial, we will fine tune a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model in the [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/). It contains 170 images with 345 instances of pedestrians, and we will use it  to train an instance segmentation model on a custom dataset.\n",
    "\n",
    "\n",
    "You will use [Azure Machine Learning Pipelines](https://aka.ms/aml-pipelines) to define two pipeline steps: a data process step which split data into training and testing, and training step which trains and evaluates the model.  The trained model then registered to your AML workspace.\n",
    "\n",
    "\n",
    "After the model is registered, you then deploy the model for serving or testing. You will deploy the model to different compute platform: 1) Azure Kubernetes Cluster (AKS), 2) your local computer\n",
    "\n",
    "This is a notebook about using ASH storage and ASH cluster (ARC compute) for training and serving, please make sure the following prerequisites are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "*     A Kubernetes cluster deployed on Azure Stack Hub, connected to Azure through ARC.\n",
    "     \n",
    "   To create a Kubernetes cluster on Azure Stack Hub, please see [here](https://docs.microsoft.com/en-us/azure-stack/user/azure-stack-kubernetes-aks-engine-overview?view=azs-2008).\n",
    "\n",
    "\n",
    "*     Connect  Azure Stack Hubâ€™s Kubernetes cluster to  Azure via [ Azure ARC](https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/connect-cluster).(For installation, please read note below first)\n",
    "    \n",
    " Important Note: This notebook  requires az extension k8s-extension >= 0.1 and connectedk8s >= 0.3.2 installed on the cluster's master node. Current version for connectedk8s in public preview release via [ Azure ARC](https://docs.microsoft.com/en-us/azure/azure-arc/kubernetes/connect-cluster) is 0.2.8, so you need to install [private preview](https://github.com/Azure/azure-arc-kubernetes-preview/blob/master/docs/k8s-extensions.md). For your convenience, we have include the wheel files, and you just need to run:\n",
    " \n",
    " <pre>\n",
    " az extension add --source connectedk8s-0.3.5-py2.py3-none-any.whl --yes\n",
    " az extension add --source k8s_extension-0.1PP.8-py2.py3-none-any.whl --yes\n",
    " </pre>\n",
    "  \n",
    "\n",
    "*     A storage account deployed on Azure Stack Hub.\n",
    "\n",
    "\n",
    "*     Setup Azure Machine Learning workspace on Azure.\n",
    "\n",
    "   please make sure the following \n",
    "   [Prerequisites](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=python#prerequisites) are met (we recommend using the Python SDK when communicating with Azure Machine Learning so make sure the SDK is properly installed). We strongly recommend learning more about the [innerworkings and concepts in Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture) before continuing with the rest of this article (optional)\n",
    "\n",
    "\n",
    "*      Last but not least, you need to be able to run a Notebook. \n",
    "\n",
    "   If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at [here](https://github.com/Azure/MachineLearningNotebooks) first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (azureml-core 1.19.0 (c:\\users\\v-songshanli\\anaconda3\\envs\\pythonproject\\lib\\site-packages), Requirement.parse('azureml-core~=1.18.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (azureml-core 1.19.0 (c:\\users\\v-songshanli\\anaconda3\\envs\\pythonproject\\lib\\site-packages), Requirement.parse('azureml-core~=1.18.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (azureml-core 1.19.0 (c:\\users\\v-songshanli\\anaconda3\\envs\\pythonproject\\lib\\site-packages), Requirement.parse('azureml-core~=1.18.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (azureml-core 1.19.0 (c:\\users\\v-songshanli\\anaconda3\\envs\\pythonproject\\lib\\site-packages), Requirement.parse('azureml-core~=1.18.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (azureml-core 1.19.0 (c:\\users\\v-songshanli\\anaconda3\\envs\\pythonproject\\lib\\site-packages), Requirement.parse('azureml-core~=1.18.0')).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azureml.core import Workspace,Environment, Experiment, Datastore\n",
    "\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core.runconfig import RunConfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "\n",
    "In preparation stage, you will create a AML workspace first, then\n",
    "\n",
    "*  Create a compute target for AML workspace by attaching the cluster deployed on Azure Stack Hub (ASH)\n",
    "\n",
    "*  Create a datastore for AML workspace backed by storage account deployed on ASH.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`. \n",
    "\n",
    "If you haven't done already please go to `config.json` file and fill in your workspace information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create Compute Target by attaching cluster deployed on ASH\n",
    "\n",
    "The attaching code here depends  python package azureml-contrib-k8s which current is in private preview. Install private preview branch of AzureML SDK by running following command (private preview):\n",
    "\n",
    "<pre>\n",
    "pip install --disable-pip-version-check --extra-index-url https://azuremlsdktestpypi.azureedge.net/azureml-contrib-k8s-preview/D58E86006C65 azureml-contrib-k8s\n",
    "</pre>\n",
    "\n",
    "Attaching ASH cluster the first time may take 7 minutes. It will be much faster after first attachment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
      "arc attach  success\n"
     ]
    }
   ],
   "source": [
    "from azureml.contrib.core.compute.arckubernetescompute import ArcKubernetesCompute\n",
    "from azureml.core import ComputeTarget\n",
    "\n",
    "resource_id = \"/subscriptions/6b736da6-3246-44dd-a0b8-b5e95484633d/resourceGroups/sl-ash2/providers/Microsoft.Kubernetes/connectedClusters/sl-d2-o-arc\"\n",
    "\n",
    "attach_config = ArcKubernetesCompute.attach_configuration(\n",
    "    resource_id= resource_id,\n",
    ")\n",
    "\n",
    "try:\n",
    "    attach_name = \"sl-d2-o-arc\"\n",
    "    arcK_target_result = ArcKubernetesCompute.attach(ws, attach_name, attach_config)\n",
    "    arcK_target_result.wait_for_completion(show_output=True)\n",
    "    print('arc attach  success')\n",
    "except ComputeTargetException as e:\n",
    "    print(e)\n",
    "    print('arc attach  failed')\n",
    "\n",
    "attach_name = \"nc6\"\n",
    "arcK_target = ComputeTarget(ws, attach_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datastore for AML workspace backed by storage account deployed on ASH\n",
    "\n",
    "Here is the [instruction](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Dataset\n",
    "\n",
    "After downloading and extracting the zip file from [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/) to your local machine, you will have the following folder structure:\n",
    "\n",
    "<pre>\n",
    "PennFudanPed/\n",
    "  PedMasks/\n",
    "    FudanPed00001_mask.png\n",
    "    FudanPed00002_mask.png\n",
    "    FudanPed00003_mask.png\n",
    "    FudanPed00004_mask.png\n",
    "    ...\n",
    "  PNGImages/\n",
    "    FudanPed00001.png\n",
    "    FudanPed00002.png\n",
    "    FudanPed00003.png\n",
    "    FudanPed00004.png\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "\n",
    "dataset_name = \"pennfudan_2\"\n",
    "datastore_name = \"ashstore\"\n",
    "datastore =  Datastore.get(ws, datastore_name)\n",
    "    \n",
    "if dataset_name not  in ws.datasets:\n",
    "    src_dir, target_path = 'PennFudanPed', 'PennFudanPed'\n",
    "    datastore.upload(src_dir, target_path)\n",
    "\n",
    "    # register data uploaded as AML dataset\n",
    "    datastore_paths = [(datastore, target_path)]\n",
    "    pd_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "    pd_ds.register(ws, dataset_name, \"for Pedestrian Detection and Segmentation\")\n",
    "    \n",
    "dataset = ws.datasets[dataset_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Training-Test split data process Step\n",
    "\n",
    "For this pipeline run, you will use two pipeline steps.  The first step is to split dataset into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class OutputFileDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class OutputDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class UploadOptions: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class OutputFileDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class OutputDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class UploadOptions: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n"
     ]
    }
   ],
   "source": [
    "# create run_config first\n",
    "\n",
    "env = Environment.from_dockerfile(\n",
    "        name='pytorch-obj-seg',\n",
    "        dockerfile='./aml_src/Dockerfile.gpu',\n",
    "        conda_specification='./aml_src/conda-env.yaml')\n",
    "\n",
    "aml_run_config = RunConfiguration()\n",
    "aml_run_config.target = arcK_target\n",
    "aml_run_config.environment = env\n",
    "\n",
    "source_directory = './aml_src'\n",
    "\n",
    "# add a data process step\n",
    "\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "dest = (datastore, None)\n",
    "\n",
    "train_split_data = OutputFileDatasetConfig(name=\"train_split_data\", destination=dest).as_upload(overwrite=False)\n",
    "test_split_data = OutputFileDatasetConfig(name=\"test_split_data\", destination=dest).as_upload(overwrite=False)\n",
    "\n",
    "split_step = PythonScriptStep(\n",
    "    name=\"Train Test Split\",\n",
    "    script_name=\"obj_segment_step_data_process.py\",\n",
    "    arguments=[\"--data-path\", dataset.as_named_input('pennfudan_data').as_mount(),\n",
    "               \"--train-split\", train_split_data, \"--test-split\", test_split_data,\n",
    "               \"--test-size\", 50],\n",
    "    compute_target=arcK_target,\n",
    "    runconfig=aml_run_config,\n",
    "    source_directory=source_directory,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = PythonScriptStep(\n",
    "        name=\"training_step\",\n",
    "        script_name=\"obj_segment_step_training.py\",\n",
    "        arguments=[\n",
    "            \"--train-split\", train_split_data.as_input(), \"--test-split\", test_split_data.as_input(),\n",
    "            '--epochs', 1,  # 80\n",
    "        ],\n",
    "\n",
    "        compute_target=arcK_target,\n",
    "        runconfig=aml_run_config,\n",
    "        source_directory=source_directory,\n",
    "        allow_reuse=True\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment and Submit Pipeline Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built.\n",
      "Created step training_step [f0f1834b][31b07c52-e930-4e62-8b05-624854c8b33e], (This step will run and generate new outputs)\n",
      "Created step Train Test Split [ae11261d][c471fce2-e506-445a-bbc7-a61e0e3913f2], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 3864b07d-0bc3-42bc-9a96-abf2b8027625\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/obj_seg_step/runs/3864b07d-0bc3-42bc-9a96-abf2b8027625?wsid=/subscriptions/6b736da6-3246-44dd-a0b8-b5e95484633d/resourcegroups/sl-ash2/workspaces/sl-ash2-mal\n",
      "PipelineRunId: 3864b07d-0bc3-42bc-9a96-abf2b8027625\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/obj_seg_step/runs/3864b07d-0bc3-42bc-9a96-abf2b8027625?wsid=/subscriptions/6b736da6-3246-44dd-a0b8-b5e95484633d/resourcegroups/sl-ash2/workspaces/sl-ash2-mal\n",
      "PipelineRun Status: Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': '3864b07d-0bc3-42bc-9a96-abf2b8027625', 'status': 'Completed', 'startTimeUtc': '2021-02-19T22:01:17.589888Z', 'endTimeUtc': '2021-02-19T22:37:42.018919Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.3864b07d-0bc3-42bc-9a96-abf2b8027625/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=OyzJd20goxYD%2FHr4vIQj50YDdEjcCx32hqZyzjihU7Q%3D&st=2021-02-19T22%3A00%3A43Z&se=2021-02-20T06%3A10%3A43Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.3864b07d-0bc3-42bc-9a96-abf2b8027625/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=PcpqzahQF833kqM8KyNHqnabjR%2B5Rfu0eDpc3jbbfK0%3D&st=2021-02-19T22%3A00%3A43Z&se=2021-02-20T06%3A10%3A43Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://slash2mal3094941854.blob.core.windows.net/azureml/ExperimentRun/dcid.3864b07d-0bc3-42bc-9a96-abf2b8027625/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=T%2F46jJaD3%2FetgHat08jFi55uUcWKufzjN%2BJgh0dt0p4%3D&st=2021-02-19T22%3A00%3A43Z&se=2021-02-20T06%3A10%3A43Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = 'obj_seg_step'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "pipeline_steps = [train_step]\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=False)\n",
    "pipeline_run.wait_for_completion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model\n",
    "\n",
    "Note: Here we saved and register two models. The model saved at \"'outputs/obj_segmentation.pkl'\" is registered as   'obj_seg_model_aml'.  It contains both model parameters and network which are used by AML deployment and serving.  Model 'obj_seg_model_kf_torch' contains only parameter values which maybe used for KFServing as shown in [this notebook](object_segmentation_kfserving.ipynb) If you are not interesting in KFServing, you can safely ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='sl-ash2-mal', subscription_id='6b736da6-3246-44dd-a0b8-b5e95484633d', resource_group='sl-ash2'), name=obj_seg_model_kf_torch, id=obj_seg_model_kf_torch:1, version=1, tags={}, properties={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step_run = pipeline_run.find_step_run(train_step.name)[0]\n",
    "\n",
    "model_name = 'obj_seg_model_aml' # model for AML serving\n",
    "train_step_run.register_model(model_name=model_name, model_path='outputs/obj_segmentation.pkl')\n",
    "\n",
    "model_name = 'obj_seg_model_kf_torch' # model for KFServing using pytorchserver\n",
    "train_step_run.register_model(model_name=model_name, model_path='outputs/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Model\n",
    "\n",
    "Here we give a few examples of deploy the model to different siturations:\n",
    "\n",
    "*   Deploy to Azure Kubernetes Cluster\n",
    "*   Deploy to local computer as a Local Docker Web Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment, Workspace, Model, ComputeTarget\n",
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to Azure Kubernetes Cluster \n",
    "\n",
    "There are two steps:\n",
    "\n",
    "1.   Create (or use existing) a AKS cluster for serving the model\n",
    "\n",
    "2.   Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provision the AKS Cluster\n",
    "\n",
    "This is a one time setup. You can reuse this cluster for multiple deployments after it has been created. If you delete the cluster or the resource group that contains it, then you would have to recreate it. It may take 5 mins to create a new AKS cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "using compute target:  aks-service-2\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose a name for your AKS cluster\n",
    "aks_name = 'aks-service-2'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
    "    is_new_compute  = False\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Use the default configuration (can also provide parameters to customize)\n",
    "    prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(workspace = ws, \n",
    "                                    name = aks_name, \n",
    "                                    provisioning_configuration = prov_config)\n",
    "    is_new_compute  = True\n",
    "    \n",
    "print(\"using compute target: \", aks_target.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running.........................................................................................................................................................................................................................\n",
      "Succeeded\n",
      "AKS service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "env = Environment.from_dockerfile(\n",
    "        name='pytorch-obj-seg',\n",
    "        dockerfile='./aml_src/Dockerfile.gpu',\n",
    "        conda_specification='./aml_src/conda-env.yaml')\n",
    "\n",
    "env.inferencing_stack_version='latest'\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=env)\n",
    "deploy_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "deployed_model =   'obj_seg_model_aml' # model_name\n",
    "model = ws.models[deployed_model]\n",
    "\n",
    "service_name = 'objservice10'\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name=service_name,\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=deploy_config,\n",
    "                       deployment_target=aks_target,\n",
    "                       overwrite=True)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Service\n",
    "\n",
    "You can \n",
    "\n",
    "*   test service directly with the service object you deployed.\n",
    "\n",
    "*   test use the restful end point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [1, 1, 1, 1, 1, 1]\n",
      "boxes [[408.70843505859375, 189.1165008544922, 530.5315551757812, 480.3606872558594], [171.4501953125, 195.43116760253906, 315.90966796875, 423.6959228515625], [432.0086364746094, 47.171043395996094, 544.4202880859375, 525.0581665039062], [343.4892272949219, 74.77059173583984, 554.1013793945312, 487.6426086425781], [115.2191162109375, 189.9022674560547, 386.2655944824219, 445.64080810546875], [185.8647918701172, 12.594765663146973, 355.4981689453125, 517.7567138671875]]\n",
      "scores [0.9914323091506958, 0.9834004044532776, 0.29651686549186707, 0.10363530367612839, 0.07148884981870651, 0.06338539719581604]\n"
     ]
    }
   ],
   "source": [
    "img_nums = [\"00001\"]\n",
    "image_paths = [\"PennFudanPed\\\\PNGImages\\\\FudanPed{}.png\".format(item) for item in img_nums]\n",
    "image_np_list = []\n",
    "for image_path in image_paths:\n",
    "    img = Image.open(image_path)\n",
    "    img.show(\"input_image\")\n",
    "    img_rgb = img.convert(\"RGB\")\n",
    "    img_tensor = F.to_tensor(img_rgb)\n",
    "    img_np = img_tensor.numpy()\n",
    "    image_np_list.append(img_np.tolist())\n",
    "\n",
    "inputs = json.dumps({\"instances\": image_np_list})\n",
    "resp = service.run(inputs)\n",
    "predicts = resp[\"predictions\"]\n",
    "\n",
    "for instance_pred in predicts:\n",
    "    print(\"labels\", instance_pred[\"labels\"])\n",
    "    print(\"boxes\", instance_pred[\"boxes\"])\n",
    "    print(\"scores\", instance_pred[\"scores\"])\n",
    "    \n",
    "    image_data = instance_pred[\"masks\"]\n",
    "    img_np = np.array(image_data)\n",
    "    output = Image.fromarray(img_np)\n",
    "    output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a  function to call the url end point\n",
    "\n",
    "Creae a simple help function to wrap the restful endpoint call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def service_infer(url, body, api_key):\n",
    "    headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
    "\n",
    "    req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "    try:\n",
    "        response = urllib.request.urlopen(req)\n",
    "\n",
    "        result = response.read()\n",
    "        return result\n",
    "\n",
    "    except urllib.error.HTTPError as error:\n",
    "        print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "        # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "        print(error.info())\n",
    "        print(json.loads(error.read().decode(\"utf8\", 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using restful end point\n",
    "\n",
    "Go to EndPonts section of your azure machine learning workspace, you will find the service you deployed. Click the service name, then click \"consume\", you will see the restful end point (the uri) and api key of your service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [1, 1, 1, 1, 1, 1]\n",
      "boxes [[408.70843505859375, 189.1165008544922, 530.5315551757812, 480.3606872558594], [171.4501953125, 195.43116760253906, 315.90966796875, 423.6959228515625], [432.0086364746094, 47.171043395996094, 544.4202880859375, 525.0581665039062], [343.4892272949219, 74.77059173583984, 554.1013793945312, 487.6426086425781], [115.2191162109375, 189.9022674560547, 386.2655944824219, 445.64080810546875], [185.8647918701172, 12.594765663146973, 355.4981689453125, 517.7567138671875]]\n",
      "scores [0.9914323091506958, 0.9834004044532776, 0.29651686549186707, 0.10363530367612839, 0.07148884981870651, 0.06338539719581604]\n"
     ]
    }
   ],
   "source": [
    "url = 'http://52.188.203.58:80/api/v1/service/objservice10/score'\n",
    "api_key = 'Gx5yYLNmsWAhAj7aYddF9l83hgVTl0Fn'  # Replace this with the API key for the web service\n",
    "\n",
    "img_nums = [\"00001\"]\n",
    "image_paths = [\"PennFudanPed\\\\PNGImages\\\\FudanPed{}.png\".format(item) for item in img_nums]\n",
    "image_np_list = []\n",
    "for image_path in image_paths:\n",
    "    img = Image.open(image_path)\n",
    "    img.show(\"input_image\")\n",
    "    img_rgb = img.convert(\"RGB\")\n",
    "    img_tensor = F.to_tensor(img_rgb)\n",
    "    img_np = img_tensor.numpy()\n",
    "    image_np_list.append(img_np.tolist())\n",
    "\n",
    "request = {\"instances\": image_np_list}\n",
    "inputs = json.dumps(request)\n",
    "\n",
    "body = str.encode(inputs)\n",
    "resp = service_infer(url, body, api_key)\n",
    "p_obj = json.loads(resp)\n",
    "\n",
    "predicts = p_obj[\"predictions\"]\n",
    "\n",
    "for instance_pred in predicts:\n",
    "    print(\"labels\", instance_pred[\"labels\"])\n",
    "    print(\"boxes\", instance_pred[\"boxes\"])\n",
    "    print(\"scores\", instance_pred[\"scores\"])\n",
    "    \n",
    "    image_data = instance_pred[\"masks\"]\n",
    "    img_np = np.array(image_data)\n",
    "    output = Image.fromarray(img_np)\n",
    "    output.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model to local computer as a Local Docker Web Service\n",
    "\n",
    "Make sure you have Docker installed and running. Note that the service creation can take few minutes.\n",
    "\n",
    "NOTE:\n",
    "\n",
    "The Docker image runs as a Linux container. If you are running Docker for Windows, you need to ensure the Linux Engine is running.\n",
    "PowerShell command to switch to Linux engine is\n",
    "\n",
    "<pre>\n",
    "& 'C:\\Program Files\\Docker\\Docker\\DockerCli.exe' -SwitchLinuxEngine\n",
    "</pre>\n",
    "\n",
    "Also, you need to login to az and acr:\n",
    "<pre>\n",
    "az login\n",
    "az acr login --name your_acr_name\n",
    "</pre>\n",
    "For more details, please see [this notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/deploy-to-local/register-model-deploy-local.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model obj_seg_model_aml:6 to C:\\Users\\V-SONG~1\\AppData\\Local\\Temp\\azureml_b5fpo65p\\obj_seg_model_aml\\6\n",
      "Generating Docker build context.\n",
      "Package creation Succeeded\n",
      "Logging into Docker registry slash2acr.azurecr.io\n",
      "Logging into Docker registry slash2acr.azurecr.io\n",
      "Building Docker image from Dockerfile...\n",
      "Step 1/5 : FROM slash2acr.azurecr.io/azureml/azureml_59da8aa0accf63910a06a4536f978b22\n",
      " ---> 74b35fe9ab8e\n",
      "Step 2/5 : COPY azureml-app /var/azureml-app\n",
      " ---> bb80a42b78b2\n",
      "Step 3/5 : RUN mkdir -p '/var/azureml-app' && echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjZiNzM2ZGE2LTMyNDYtNDRkZC1hMGI4LWI1ZTk1NDg0NjMzZCIsInJlc291cmNlR3JvdXBOYW1lIjoic2wtYXNoMiIsImFjY291bnROYW1lIjoic2wtYXNoMi1tYWwiLCJ3b3Jrc3BhY2VJZCI6IjFhM2ZmOTQwLTI1OWItNGJlMi05NjE1LTgwY2Y1ZWUzY2VlNyJ9LCJtb2RlbHMiOnt9LCJtb2RlbHNJbmZvIjp7fX0= | base64 --decode > /var/azureml-app/model_config_map.json\n",
      " ---> Running in d73cf9dd74da\n",
      " ---> 5565d606ca0c\n",
      "Step 4/5 : RUN mv '/var/azureml-app/tmpm5x9bplw.py' /var/azureml-app/main.py\n",
      " ---> Running in 7386be2511b9\n",
      " ---> c4bf4a97a786\n",
      "Step 5/5 : CMD [\"runsvdir\",\"/var/runit\"]\n",
      " ---> Running in 428fee12a44b\n",
      " ---> db7c89bbe7e6\n",
      "Successfully built db7c89bbe7e6\n",
      "Successfully tagged localtest:latest\n",
      "Container (name:musing_easley, id:084e5b51cab9f5cdef72d05a6c6400eba07ee0edb0497b833e9d453c34f21236) cannot be killed.\n",
      "Container has been successfully cleaned up.\n",
      "Image sha256:201cde67c672cadf8429fc37ccc9c046f2eef47dc5eca3bb03ff8b11c0b070d3 successfully removed.\n",
      "Starting Docker container...\n",
      "Docker container running.\n",
      "Checking container health...\n",
      "Local webservice is running at http://localhost:6789\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# This is optional, if not provided Docker will choose a random unused port.\n",
    "deployment_config = LocalWebservice.deploy_configuration(port=6789)\n",
    "\n",
    "deployed_model = \"obj_seg_model_aml\" # model_name\n",
    "model = ws.models[deployed_model]\n",
    "\n",
    "local_service = Model.deploy(ws, \"localtest\", [model], inference_config, deployment_config)\n",
    "\n",
    "local_service.wait_for_deployment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local service port: 6789\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Local service port: {}'.format(local_service.port))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Status and Get Container Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-20T00:59:44,740945400+00:00 - gunicorn/run \n",
      "2021-02-20T00:59:44,740999800+00:00 - iot-server/run \n",
      "2021-02-20T00:59:44,740945300+00:00 - rsyslog/run \n",
      "2021-02-20T00:59:44,745501000+00:00 - nginx/run \n",
      "rsyslogd: /azureml-envs/azureml_70c4c8d68a5830d13428563ac210102e/lib/libuuid.so.1: no version information available (required by rsyslogd)\n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2021-02-20T00:59:44,818951900+00:00 - iot-server/finish 1 0\n",
      "2021-02-20T00:59:44,820150900+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 19.9.0\n",
      "Listening at: http://127.0.0.1:31311 (13)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 46\n",
      "SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "Initializing logger\n",
      "2021-02-20 00:59:45,356 | root | INFO | Starting up app insights client\n",
      "2021-02-20 00:59:45,357 | root | INFO | Starting up request id generator\n",
      "2021-02-20 00:59:45,357 | root | INFO | Starting up app insight hooks\n",
      "2021-02-20 00:59:45,357 | root | INFO | Invoking user's init function\n",
      "2021-02-20 00:59:45,641 | root | INFO | Users's init has completed successfully\n",
      "2021-02-20 00:59:45,643 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2021-02-20 00:59:45,643 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2021-02-20 00:59:45,644 | root | INFO | Scoring timeout setting is not found. Use default timeout: 3600000 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(local_service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test local deployment\n",
    "\n",
    "You can use local_service.run to test your deployment as shown in  case of deployment to Kubernetes Cluster. You can use end point to test your inference service as well.  In this local deployment, the endpoint is http://localhost:{port}/score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "pythonproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
